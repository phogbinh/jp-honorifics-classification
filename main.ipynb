{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d9e680",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8963df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-v2\"\n",
    "MODEL_LEARNING_RATE = 1e-5\n",
    "MODEL_BATCH_SIZE = 128\n",
    "MODEL_EPOCHS_NUM = 10\n",
    "MODEL_OUTPUT_DIR = \"./trained_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15887196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff8c7e5",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9827e3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-be7e0937d2f084e2\n",
      "Found cached dataset csv (/home/bill/.cache/huggingface/datasets/csv/default-be7e0937d2f084e2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23a397403a744dc877696dc87242407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['本文', 'Level', '尊敬語', '謙譲語', '丁寧語', 'フィールド'],\n",
       "        num_rows: 10007\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"data\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2eeda5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>本文</th>\n",
       "      <th>Level</th>\n",
       "      <th>尊敬語</th>\n",
       "      <th>謙譲語</th>\n",
       "      <th>丁寧語</th>\n",
       "      <th>フィールド</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>御病気なさったそうですね。回復されたのでしょうか？</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>挨拶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>本日は温かい気候ですね。</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>挨拶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>行ってらっしゃいませ。お気を付けて。</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>挨拶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>皆様、御揃いでお出かけになられるのですか？</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>挨拶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>本日はお早いのですね、お散歩ですか？</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>挨拶</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          本文  Level  尊敬語  謙譲語  丁寧語 フィールド\n",
       "0  御病気なさったそうですね。回復されたのでしょうか？      1    1    0    0    挨拶\n",
       "1               本日は温かい気候ですね。      1    1    0    0    挨拶\n",
       "2         行ってらっしゃいませ。お気を付けて。      1    1    0    0    挨拶\n",
       "3      皆様、御揃いでお出かけになられるのですか？      1    1    0    0    挨拶\n",
       "4         本日はお早いのですね、お散歩ですか？      1    1    0    0    挨拶"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca9481",
   "metadata": {},
   "source": [
    "# Set tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334a4259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 2828, 6855, 897, 3303, 13231, 15652, 12461, 895, 829, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer(\"本日は温かい気候ですね。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16b582",
   "metadata": {},
   "source": [
    "# Set encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84607de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Level\n",
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"Level\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb2abfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3, 4])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder().fit(df[[\"Level\"]])\n",
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "019f12d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.transform(pd.DataFrame({\"Level\": [3]})).toarray()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416173f",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed75f8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ab43fda1f643318e4e5bc8c998e1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['本文', 'Level', '尊敬語', '謙譲語', '丁寧語', 'フィールド', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 10007\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(dataslice):\n",
    "  tokenized_inputs = tokenizer(dataslice[\"本文\"])\n",
    "  labels = []\n",
    "  for level in dataslice[\"Level\"]:\n",
    "    encoded_level = encoder.transform(pd.DataFrame({\"Level\": [level]})).toarray()[0]\n",
    "    labels.append(encoded_level)\n",
    "  tokenized_inputs[\"label\"] = labels\n",
    "  return tokenized_inputs\n",
    "\n",
    "processed_dataset = dataset.map(preprocess, batched=True)\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f0d45e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'本文': '本日は温かい気候ですね。',\n",
       " 'Level': 1,\n",
       " '尊敬語': 1,\n",
       " '謙譲語': 0,\n",
       " '丁寧語': 0,\n",
       " 'フィールド': '挨拶',\n",
       " 'input_ids': [2, 2828, 6855, 897, 3303, 13231, 15652, 12461, 895, 829, 3],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'label': [1.0, 0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72fadb0",
   "metadata": {},
   "source": [
    "# Set categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08fecb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = encoder.categories_[0]\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23db893",
   "metadata": {},
   "source": [
    "# Set model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18372bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1987f96f",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63de3274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['本文', 'Level', '尊敬語', '謙譲語', '丁寧語', 'フィールド', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 9006\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['本文', 'Level', '尊敬語', '謙譲語', '丁寧語', 'フィールド', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 1001\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eval_dataset = processed_dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "train_eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb26b154",
   "metadata": {},
   "source": [
    "# Set trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34dbe0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=MODEL_OUTPUT_DIR,\n",
    "  learning_rate=MODEL_LEARNING_RATE,\n",
    "  per_device_train_batch_size=MODEL_BATCH_SIZE,\n",
    "  per_device_eval_batch_size=MODEL_BATCH_SIZE,\n",
    "  num_train_epochs=MODEL_EPOCHS_NUM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1751382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=train_eval_dataset[\"train\"],\n",
    "  eval_dataset=train_eval_dataset[\"test\"],\n",
    "  tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d882c541",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d6f3e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: 本文, 謙譲語, フィールド, 尊敬語, 丁寧語, Level. If 本文, 謙譲語, フィールド, 尊敬語, 丁寧語, Level are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/bill/miniconda3/envs/jpenv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9006\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 710\n",
      "  Number of trainable parameters = 111210244\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='710' max='710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [710/710 04:13, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.295600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./trained_model/checkpoint-500\n",
      "Configuration saved in ./trained_model/checkpoint-500/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model = True\n",
    "if train_model:\n",
    "  trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4608963c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./trained_model/config.json\n",
      "Model weights saved in ./trained_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "save_model = True\n",
    "if save_model:\n",
    "  model.save_pretrained(MODEL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c697b5b",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abb1c4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./trained_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese-v2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file ./trained_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./trained_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "trained_model = BertForSequenceClassification.from_pretrained(MODEL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "218f641a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['あなたの言葉も納得出来ます。',\n",
       " '不出来な息子ですが、よろしく指導してください。',\n",
       " '毎度！ご贔屓頂戴いたしまして、ありがとう存じます。',\n",
       " 'お困りのようなので、お貸ししましょう。',\n",
       " 'そのためには、安定した政府の樹立が不可欠だと考えています。']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts = train_eval_dataset[\"test\"][\"本文\"]\n",
    "test_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f3b6d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.4256, -3.6581,  2.8488, -2.6851],\n",
       "        [-3.1693, -2.7126,  2.9857, -3.5900],\n",
       "        [ 2.0378, -1.8804, -3.9944, -4.5573],\n",
       "        [-1.5672,  1.1138, -2.3519, -4.9037],\n",
       "        [ 0.1852, -3.3480,  0.0877, -3.9275]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = tokenizer(test_texts, padding=True, return_tensors=\"pt\")\n",
    "logits = trained_model(**model_input).logits\n",
    "logits[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "509ea16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0019, 0.0015, 0.9927, 0.0039],\n",
       "        [0.0021, 0.0033, 0.9932, 0.0014],\n",
       "        [0.9769, 0.0194, 0.0023, 0.0013],\n",
       "        [0.0621, 0.9073, 0.0284, 0.0022],\n",
       "        [0.5121, 0.0150, 0.4645, 0.0084]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "predicted_probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "predicted_probabilities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13f6bce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 1, 2, 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted_labels = [categories[label_idx]\n",
    "                    for label_idx in np.argmax(predicted_probabilities.detach().numpy(), axis=1)]\n",
    "predicted_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa189283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: あなたの言葉も納得出来ます。\n",
      "3: 不出来な息子ですが、よろしく指導してください。\n",
      "1: 毎度！ご贔屓頂戴いたしまして、ありがとう存じます。\n",
      "2: お困りのようなので、お貸ししましょう。\n",
      "1: そのためには、安定した政府の樹立が不可欠だと考えています。\n",
      "4: 電話口まで来てもらって悪いね。今大丈夫？\n",
      "4: 弘法大師さんを信仰しているの？\n",
      "2: 先輩から、お杯をいただいてしまったわ。\n",
      "1: ご子息は？／ご子息は何人いらっしゃるんですか？\n",
      "3: おじは、麻雀に夢中になってしまい落第してしまったそうです。\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "  print(f\"{predicted_labels[idx]}: {test_texts[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce7ed462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7542457542457542"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrects_num = 0\n",
    "for idx in range(len(predicted_labels)):\n",
    "  if predicted_labels[idx] == train_eval_dataset[\"test\"][\"Level\"][idx]:\n",
    "    corrects_num = corrects_num + 1\n",
    "corrects_num / len(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
